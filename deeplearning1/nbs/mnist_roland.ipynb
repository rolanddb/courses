{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import * \n",
    "from __future__ import division, print_function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "cell to markdown: 'm', cell to code: 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-73020798007e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[:10000]\n",
    "y_train=y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the input space ==> 28 * 28 * 1 (grayscale)\n",
    "(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the data shape - it has one channel (grayscale) and we need to specify this explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, 1)\n",
    "X_test = np.expand_dims(X_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the labels look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use onehot encoding for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(y_train)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = onehot(y_train)\n",
    "y_test = onehot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_model(): \n",
    "    model = Sequential([\n",
    "        # The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.\n",
    "        Lambda(dummy, input_shape=(1, 28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # Adam is an optimizer: https://keras.io/optimizers/#adam\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_1_model\" was not an Input tensor, it was generated by layer lambda_1.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_1\n",
      "  str(x.name))\n"
     ]
    }
   ],
   "source": [
    "lm = get_linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 1, 28, 28)     0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 784)           0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            7850        flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by training fast for 1 iteration, then slow down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 0s - loss: 14.1676 - acc: 0.1208 - val_loss: 14.1638 - val_acc: 0.1212\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 0s - loss: 14.1005 - acc: 0.1251 - val_loss: 13.9144 - val_acc: 0.1366\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.7618 - acc: 0.1461 - val_loss: 13.8712 - val_acc: 0.1393\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11aec0490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr=0.1\n",
    "lm.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 0s - loss: 13.8567 - acc: 0.1403 - val_loss: 13.8714 - val_acc: 0.1393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11aa7c3d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr=0.01\n",
    "lm.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), nb_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Met de imagegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow(X_train, y_train, batch_size=64)\n",
    "test_batches = gen.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/topology.py:1811: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_2_model\" was not an Input tensor, it was generated by layer lambda_2.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_2\n",
      "  str(x.name))\n"
     ]
    }
   ],
   "source": [
    "lm_img_gen = get_linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s - loss: 12.7013 - acc: 0.2117 - val_loss: 12.7818 - val_acc: 0.2069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b61f250>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_img_gen.optimizer.lr=0.1\n",
    "lm_img_gen.fit_generator(batches, batches.n, nb_epoch=1, \n",
    "                    validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze performance is waardeloos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel = X_train.mean().astype(np.float32)\n",
    "std_pixel = X_train.std().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(x):\n",
    "    return ((x - mean_pixel) / std_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linair_model_normalized(): \n",
    "    model = Sequential([\n",
    "        # The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.\n",
    "        Lambda(normalize_input, input_shape=(1, 28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # Adam is an optimizer: https://keras.io/optimizers/#adam\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_3 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "lmnorm = get_linair_model_normalized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 0s - loss: 11.3982 - acc: 0.2850 - val_loss: 11.4692 - val_acc: 0.2867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11556a690>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmnorm.optimizer.lr=0.1\n",
    "lmnorm.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 0s - loss: 11.3981 - acc: 0.2918 - val_loss: 11.4151 - val_acc: 0.2912\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 0s - loss: 11.3602 - acc: 0.2941 - val_loss: 11.4190 - val_acc: 0.2910\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 0s - loss: 11.3637 - acc: 0.2944 - val_loss: 11.5009 - val_acc: 0.2854\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 0s - loss: 11.3622 - acc: 0.2945 - val_loss: 11.5165 - val_acc: 0.2847\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 0s - loss: 11.3619 - acc: 0.2942 - val_loss: 11.3823 - val_acc: 0.2933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11556a650>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmnorm.optimizer.lr=0.01\n",
    "lmnorm.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), nb_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow(X_train, y_train, batch_size=64)\n",
    "test_batches = gen.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_4 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "lm3 = get_linair_model_normalized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op de een of andere manier verbetert de image generator de performance dramatisch, maar ik snap ff niet waarom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s - loss: 8.1672 - acc: 0.4806 - val_loss: 8.2354 - val_acc: 0.4854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115b7afd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.optimizer.lr=0.1\n",
    "lm3.fit_generator(batches, batches.n, nb_epoch=1, \n",
    "                    validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s - loss: 8.1172 - acc: 0.4936 - val_loss: 8.2367 - val_acc: 0.4866\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s - loss: 8.0634 - acc: 0.4978 - val_loss: 8.1732 - val_acc: 0.4912\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s - loss: 8.0022 - acc: 0.5012 - val_loss: 8.2052 - val_acc: 0.4890\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s - loss: 7.9795 - acc: 0.5033 - val_loss: 8.2258 - val_acc: 0.4877\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s - loss: 8.0738 - acc: 0.4974 - val_loss: 8.1678 - val_acc: 0.4921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116a0fcd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.optimizer.lr=0.01\n",
    "lm3.fit_generator(batches, batches.n, nb_epoch=5, \n",
    "                    validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected model (has dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_model(): \n",
    "    model = Sequential([\n",
    "        # The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.\n",
    "        Lambda(normalize_input, input_shape=(1, 28, 28)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='softmax'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # Adam is an optimizer: https://keras.io/optimizers/#adam\n",
    "    # This is a multiclass problem, so we use categorical_crossentropy rather than binary_crossentropy\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_5 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "fc_model = get_fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.1130 - acc: 0.7320 - val_loss: 1.9651 - val_acc: 0.8312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117839b50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit_generator(batches, batches.n, nb_epoch=1, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 1.8315 - acc: 0.8644 - val_loss: 1.7091 - val_acc: 0.8799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117978210>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr=0.1\n",
    "fc_model.fit_generator(batches, batches.n, nb_epoch=1, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s - loss: 0.8024 - acc: 0.9427 - val_loss: 0.8075 - val_acc: 0.9146\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s - loss: 0.7029 - acc: 0.9495 - val_loss: 0.7281 - val_acc: 0.9176\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s - loss: 0.6243 - acc: 0.9523 - val_loss: 0.6621 - val_acc: 0.9184\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 3s - loss: 0.5598 - acc: 0.9527 - val_loss: 0.6089 - val_acc: 0.9171\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 3s - loss: 0.4996 - acc: 0.9573 - val_loss: 0.5618 - val_acc: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117978290>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr=0.01\n",
    "fc_model.fit_generator(batches, batches.n, nb_epoch=5, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(): \n",
    "    model = Sequential([\n",
    "        # The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.\n",
    "        Lambda(normalize_input, input_shape=(1, 28, 28)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "#         Convolution2D(64,3,3, activation='relu'),\n",
    "#         Convolution2D(64,3,3, activation='relu'),\n",
    "#         MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='softmax'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # Adam is an optimizer: https://keras.io/optimizers/#adam\n",
    "    # This is a multiclass problem, so we use categorical_crossentropy rather than binary_crossentropy\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_6 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 1, 28, 28)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "cnn_model = get_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 63s - loss: 1.8755 - acc: 0.8008 - val_loss: 1.7593 - val_acc: 0.8164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117e19c10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit_generator(batches, batches.n, nb_epoch=1, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 45s - loss: 1.6532 - acc: 0.8373 - val_loss: 1.5406 - val_acc: 0.8583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1196efd50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.optimizer.lr=0.1\n",
    "cnn_model.fit_generator(batches, batches.n, nb_epoch=1, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 27s - loss: 1.2482 - acc: 0.8779 - val_loss: 1.1812 - val_acc: 0.8665\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 28s - loss: 1.0891 - acc: 0.8785 - val_loss: 1.0279 - val_acc: 0.8736\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 29s - loss: 0.9469 - acc: 0.8821 - val_loss: 0.9047 - val_acc: 0.8741\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 29s - loss: 0.8272 - acc: 0.8884 - val_loss: 0.8011 - val_acc: 0.8761\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 29s - loss: 0.7368 - acc: 0.8842 - val_loss: 0.7211 - val_acc: 0.8777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1196ef350>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.optimizer.lr=0.01\n",
    "cnn_model.fit_generator(batches, batches.n, nb_epoch=5, validation_data=test_batches, nb_val_samples=test_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/ \n",
    "The training dataset is structured as a 3-dimensional array of instance, image width and image height. For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values.\n",
    "\n",
    "We can do this transform easily using the reshape() function on the NumPy array. We can also reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "(X_train2, y_train2), (X_test2, y_test2) = mnist.load_data()\n",
    "num_pixels = X_train2.shape[1] * X_train2.shape[2]\n",
    "X_train_flat = X_train2.reshape(X_train2.shape[0], num_pixels).astype('float32')\n",
    "X_test_flat = X_test2.reshape(X_test2.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train2 = X_train2 / 255\n",
    "X_test2 = X_test2 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train2 = np_utils.to_categorical(y_train2)\n",
    "y_test2 = np_utils.to_categorical(y_test2)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.\n",
    "\n",
    "A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction. Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights.\n",
    "\n",
    "We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected dense_input_4 to have 2 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-06e580eccf6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m   1030\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roland/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: expected dense_input_4 to have 2 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train2, y_train2, validation_data=(X_test2, y_test2), nb_epoch=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test2, y_test2, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
